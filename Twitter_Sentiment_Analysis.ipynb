{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "92b885dd147dac19bd0a33db3cd0da100bd5bc23",
        "id": "u-SFdWFtLUbV"
      },
      "source": [
        "# Twitter Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcoNiGXVN8zy",
        "outputId": "8ca70c3d-e09c-4377-fb28-da52f1cc030f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3-pECJbOM69",
        "outputId": "1a85a442-c488-49ee-87d4-98c3c24dce0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wNbFzrRPVtl",
        "outputId": "e91ced37-c561-4d41-9e4d-cc004527ad3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkdZoXPSPby4",
        "outputId": "bed21751-e614-4c80-d723-f3d1ec7f5e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
            "License(s): other\n",
            "sentiment140.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Niqj_7fpPpzX",
        "outputId": "a582e287-f631-4da8-f984-9f0d8d9641d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/sentiment140.zip\n",
            "replace training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/sentiment140.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_uuid": "70282bce8b42a51e4d44f2c7d85c4ca9567b0fd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-07T15:12:33.947796Z",
          "iopub.status.busy": "2025-03-07T15:12:33.947578Z"
        },
        "id": "yZI7rWTRLUbX",
        "outputId": "33ce1ef5-8528-4b1e-e213-8382a3c00629",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.9.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim --upgrade\n",
        "!pip install keras --upgrade\n",
        "!pip install pandas --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8dZk98cLrTh",
        "outputId": "5dda47e7-f1d8-4894-aa71-28d3f5b66ba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.11/dist-packages (2.18.1)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-text) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.9.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "_uuid": "303e72966af732ddef0bd8108a321095314e44af",
        "id": "nEHfbzCILUbY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import itertools\n",
        "\n",
        "# Set log\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "_uuid": "35e1a89dead5fd160e4c9a024a21d2e569fc89ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RcNDo8CLUba",
        "outputId": "570f4ad7-aef8-4b48-a9f8-f44a672e6638",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e8b01a07df001e4abcc745900336c4db06e455f3",
        "id": "Knbgp2GsLUbc"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0tKuQE2kMJLY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "_uuid": "180f0dd2a95419e4602b5c0229822b0111c826f6",
        "id": "KoIgh9baLUbe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# DATASET\n",
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# WORD2VEC\n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1c3beecc618be68480b3d4f0de08d9d863da1dc1",
        "id": "VE9LX6haLUbh"
      },
      "source": [
        "### Read Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "563b3c44f1092dba0b853747b098e00509098cca",
        "id": "z6ICwwzILUbk"
      },
      "source": [
        "### Dataset details\n",
        "* **target**: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "* **ids**: The id of the tweet ( 2087)\n",
        "* **date**: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "* **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "* **user**: the user that tweeted (robotickilldozr)\n",
        "* **text**: the text of the tweet (Lyx is cool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0I1wVODVNQpE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/training.1600000.processed.noemoticon.csv\", encoding=\"ISO-8859-1\")\n",
        "# Added encoding=\"ISO-8859-1\" to specify the correct file encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "oKYWL6n7f8fv",
        "outputId": "3a998966-32fd-4cad-9b65-33b768bb4a52"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b8a4f4996874>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Adjust n_samples_per_class to have 20,000 total samples (e.g., 10,000 per class if you have 2 classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mn_samples_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m  \u001b[0;31m# Example for 2 classes, resulting in 20,000 total samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdownsampled_df_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownsample_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples_per_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Now, update x_train and y_train based on the downsampled DataFrame:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "def downsample_data(df, target_col, n_samples_per_class):\n",
        "    \"\"\"Downsamples data to have n_samples_per_class for each class.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame to downsample.\n",
        "        target_col (str): Column name containing class labels.\n",
        "        n_samples_per_class (int): Number of samples per class.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Downsampled DataFrame.\n",
        "    \"\"\"\n",
        "    unique_classes = df[target_col].unique()\n",
        "    downsampled_df = pd.DataFrame()\n",
        "    for class_label in unique_classes:\n",
        "        class_data = df[df[target_col] == class_label]\n",
        "        downsampled_class = resample(class_data,\n",
        "                                     replace=False,  # Without replacement to avoid duplicates\n",
        "                                     n_samples=n_samples_per_class,\n",
        "                                     random_state=42)  # For reproducibility\n",
        "        downsampled_df = pd.concat([downsampled_df, downsampled_class])\n",
        "    return downsampled_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle and reset index\n",
        "\n",
        "# Assuming df_train contains your training data and 'target' column has the labels\n",
        "# Adjust n_samples_per_class to have 20,000 total samples (e.g., 10,000 per class if you have 2 classes)\n",
        "n_samples_per_class = 10000  # Example for 2 classes, resulting in 20,000 total samples\n",
        "downsampled_df_train = downsample_data(df_train, 'target', n_samples_per_class)\n",
        "\n",
        "# Now, update x_train and y_train based on the downsampled DataFrame:\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(downsampled_df_train.text), maxlen=SEQUENCE_LENGTH)\n",
        "y_train = encoder.transform(downsampled_df_train.target.tolist())\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "print(\"New x_train shape:\", x_train.shape)\n",
        "print(\"New y_train shape:\", y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "5uT0QcJXQDM8",
        "outputId": "38f5c327-2b44-4284-efde-fd60f17e234d"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'target'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'target'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-656906254106>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'target'"
          ]
        }
      ],
      "source": [
        "df['target'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "936d499c00c4f1648bc16ca9d283c3b39be7fb10",
        "id": "ZBRmXhi-LUbn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(\"Dataset size:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "7486ed895b813c5246f97b31b6162b0f65ff763b",
        "id": "WvEgX2mkLUbo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3f9a7bb129e184967b13261fb5d253af451c75c5",
        "id": "1XlsOD_dLUbp"
      },
      "source": [
        "### Map target label to String\n",
        "* **0** -> **NEGATIVE**\n",
        "* **2** -> **NEUTRAL**\n",
        "* **4** -> **POSITIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "14074b59106cb9550440839e48b832223fc9502f",
        "id": "KIj0s2w_LUbp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\n",
        "def decode_sentiment(label):\n",
        "    return decode_map[int(label)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "4449d473187f647a195a6ac6986b009da32a7f4b",
        "id": "ACCXKDlVLUbq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df.target = df.target.apply(lambda x: decode_sentiment(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "19eb327803192f31cce3512aacb232f4d6b38715",
        "id": "zbv12ZL7LUbq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "target_cnt = Counter(df.target)\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.bar(target_cnt.keys(), target_cnt.values())\n",
        "plt.title(\"Dataset labels distribuition\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4329b1573518b03e497213efa7676220734ebb4b",
        "id": "TaLAlRC2LUbr"
      },
      "source": [
        "### Pre-Process dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "8aeee8b7b9ea11b749c7f91cd4787a7b50ed1a91",
        "id": "lUQLaFCcLUbs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "649ebcb97969b9ac4301138783704bb3d7846a49",
        "id": "N-PGJ8o1LUbs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "f7f3e77ab9291d14687c49e71ba9b2b1e3323432",
        "id": "eRQcbORaLUbs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df.text = df.text.apply(lambda x: preprocess(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f5f9714a8507409bbe780eebf2855a33e8e6ba37",
        "id": "FJmypRfDLUbs",
        "trusted": true
      },
      "source": [
        "### Split train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "d2b1179c968e3f3910c790ecf0c5b2cbb34b0e68",
        "id": "v1v2-1hCLUbt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f08a28aab2c3d16d8b9681a7d5d07587153a1cd6",
        "id": "yuCwMjMOLUbt"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "2461bf564de1b4414841933d0c1d1bee5f5cc5a6",
        "id": "mK7R-sqaLUbt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "documents = [_text.split() for _text in df_train.text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "8e19b9f25801ba86420decc266d2b3e6fb44f1ea",
        "id": "F5fCUHTrLUbu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE,\n",
        "                                            window=W2V_WINDOW,\n",
        "                                            min_count=W2V_MIN_COUNT,\n",
        "                                            workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "58d655af07653c594bec6bebcfb302a973b0ad9c",
        "id": "_CB5XJ5QLUbu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "w2v_model.build_vocab(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "72a5628ca81fd4b8983c12d93ae0bf950b86b6ae",
        "id": "39GhfAcbLUbu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "words = list(w2v_model.wv.key_to_index.keys()) # Use key_to_index instead of vocab\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "68c3e4a5ba07cac3dee67f78ecdd1404c7f83f14",
        "id": "vRYRGxyrLUbu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "27cc2651c74227115d8bfd8c40e5618048e05edd",
        "id": "hUiQDJZJLUbv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.most_similar(\"love\") # Access most_similar through the wv attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e13563644468037258598637b49373ca96b9b879",
        "id": "z__Ajq2ZLUbv"
      },
      "source": [
        "### Tokenize Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "6852bc709a7cd20173cbeeb218505078f8f37c57",
        "id": "Yd6-Gn-ALUbv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train.text)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Total words\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "45de439df3015030c71f84c2d170346936a1d68f",
        "id": "WVZoax-ELUbw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "03b35903fc6260e190d6928d240ef7432de117fc",
        "id": "6qY3WjxkLUbw"
      },
      "source": [
        "### Label Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "33676e0efa39e97d89bd650b8b4eae933a22fbf0",
        "id": "Lx4TVZPvLUbw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "labels = df_train.target.unique().tolist()\n",
        "labels.append(NEUTRAL)\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "04239a9bef76e7922fd86098a5601dfde8ee4665",
        "id": "j2Za7YwWLUbx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_train.target.tolist())\n",
        "\n",
        "y_train = encoder.transform(df_train.target.tolist())\n",
        "y_test = encoder.transform(df_test.target.tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(\"y_train\",y_train.shape)\n",
        "print(\"y_test\",y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "04299c886911ca135583ab64878f213939a2990c",
        "id": "K-PKujU1LUbx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(\"x_train\", x_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print()\n",
        "print(\"x_test\", x_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "232533fb27b7be99d9b8c2f8fb22c9c6bf121a6f",
        "id": "BxjEXjjoLUby",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "233c0ea94055a03e2e7df3e2a13d036ec963484f",
        "id": "JvdAJNbBLUby"
      },
      "source": [
        "### Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "9ab488374b59e3f30f8b1ea92767d853c4846bac",
        "id": "u7ffXbFHLUby",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word in w2v_model.wv:\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "833279d91e4286065968237fb5f2a0c2dd4d246c",
        "id": "AcvPQODbLUbz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b299ef78f94c2085942c993a2d58753a7476305a",
        "id": "E_ilGt6ZLUbz"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "e775ef4f1b74e6412457181383c39f2df554ef3f",
        "id": "pgox82fYLUb0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "28d22eafd0c7d798dcf3d742bc92fb8577939e6c",
        "id": "X-USCrY8LUb0"
      },
      "source": [
        "### Compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "1331e08d590bb2aa2033706c8faca217afc0f1c3",
        "id": "FjAATLosLUb0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c7733127cb8b380e0c807268903bf4d03ef92542",
        "id": "Fx9wP38xLUb0"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "a688df590386f5748da6fe00b01904fe6c71619e",
        "id": "uuxBNo28LUb1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
        "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5, mode='max')] # Added mode='max'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8d0873633dd49179c8cae17377641b97d323ef3b",
        "id": "IjXvYlHZLUb1"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3zN6UQUVWj8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "def downsample_data(df, target_col, n_samples_per_class):\n",
        "    \"\"\"Downsamples data to have n_samples_per_class for each class.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame to downsample.\n",
        "        target_col (str): Column name containing class labels.\n",
        "        n_samples_per_class (int): Number of samples per class.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Downsampled DataFrame.\n",
        "    \"\"\"\n",
        "    unique_classes = df[target_col].unique()\n",
        "    downsampled_df = pd.DataFrame()\n",
        "    for class_label in unique_classes:\n",
        "        class_data = df[df[target_col] == class_label]\n",
        "        downsampled_class = resample(class_data,\n",
        "                                     replace=False,  # Without replacement to avoid duplicates\n",
        "                                     n_samples=n_samples_per_class,\n",
        "                                     random_state=42)  # For reproducibility\n",
        "        downsampled_df = pd.concat([downsampled_df, downsampled_class])\n",
        "    return downsampled_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle and reset index\n",
        "\n",
        "# Assuming df_train contains your training data and 'target' column has the labels\n",
        "# Adjust n_samples_per_class to have 20,000 total samples (e.g., 10,000 per class if you have 2 classes)\n",
        "n_samples_per_class = 10000  # Example for 2 classes, resulting in 20,000 total samples\n",
        "downsampled_df_train = downsample_data(df_train, 'target', n_samples_per_class)\n",
        "\n",
        "# Now, update x_train and y_train based on the downsampled DataFrame:\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(downsampled_df_train.text), maxlen=SEQUENCE_LENGTH)\n",
        "y_train = encoder.transform(downsampled_df_train.target.tolist())\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "print(\"New x_train shape:\", x_train.shape)\n",
        "print(\"New y_train shape:\", y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6HC7REnV-mp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "2b659d390c6577dc5cdb6b6297934279b4e801d5",
        "id": "GI5qfcSqLUb1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "%%time\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=4,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "267258196d96796ac69a7b8c466314bcf5d6ee42",
        "id": "2jIAk1FxLUb2"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "98ecd8f1b8b74594c3ea775dd68a094e92458022",
        "id": "FhyaIgAzLUb2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
        "print()\n",
        "print(\"ACCURACY:\",score[1])\n",
        "print(\"LOSS:\",score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "40c72cd1e9d6c4fd799cbba7c813765ac4039dfc",
        "id": "cj7FRB8-LUb3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6bdfc0f6a6af5bebc0271d83dd7432c91001409b",
        "id": "6LHbJHMxLUb3"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "f0b0fa3d4b1bb14b3f5e3d169a369f3ebef29ae1",
        "id": "lvpte15JLUb3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def decode_sentiment(score, include_neutral=True):\n",
        "    if include_neutral:\n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "ed4086d651f2f8cbed11d3c909a8873607d29a06",
        "id": "SL_R7wU8LUb4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def predict(text, include_neutral=True):\n",
        "    start_at = time.time()\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "    # Predict\n",
        "    score = model.predict([x_test])[0]\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "\n",
        "    return {\"label\": label, \"score\": float(score),\n",
        "       \"elapsed_time\": time.time()-start_at}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "ca38b1e6c9b5acfed7467de2cf02a78333108872",
        "id": "Uq9qzcAJLUb4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "predict(\"I love the music\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "0e5fe647533be0148850de349fea6ef6f71303d1",
        "id": "R0NvpOxbLUb4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "predict(\"I hate the rain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "37064dffcc8920d34ccd54fac7c8b50e583a8269",
        "id": "DgEpJnE5LUb4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "predict(\"i don't know what i'm doing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3ee72e47f84b6dbc32e02a783de5ec1661f157e1",
        "id": "9mYvqQZqLUb5"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "0e920173eb05f04aecdd735bc5dff0f5be5f8d15",
        "id": "A_bNNmljLUb5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "y_pred_1d = []\n",
        "y_test_1d = list(df_test.target)\n",
        "scores = model.predict(x_test, verbose=1, batch_size=80)\n",
        "y_pred_1d = [decode_sentiment(score, include_neutral=False) for score in scores]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "b3575191bb425ab871f3f41e83812ee84bb7e595",
        "id": "RNwMnAUeLUb5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=30)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n",
        "    plt.yticks(tick_marks, classes, fontsize=22)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label', fontsize=25)\n",
        "    plt.xlabel('Predicted label', fontsize=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "a57dc6f6211c144491a70f533225edfa95a2dc66",
        "id": "AJBS-TPaLUb5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\n",
        "plt.figure(figsize=(12,12))\n",
        "plot_confusion_matrix(cnf_matrix, classes=df_train.target.unique(), title=\"Confusion matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e23b957348dcc084249d3cc7538b972da471c2cd",
        "id": "nM3gAbFbLUb6"
      },
      "source": [
        "### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "a7fe05b7caa1c984ff1deb0be2f7c6bc043df9f5",
        "id": "779lkptYLUb6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test_1d, y_pred_1d))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4eb300f0c6693a618587c7dcf32f77f5416cbfb9",
        "id": "EK_vajQBLUb6"
      },
      "source": [
        "### Accuracy Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "5cf76e6e09f8a60ed25947932b94c772eda44d23",
        "id": "fv0vF4Z0LUb7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test_1d, y_pred_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4f014c32f3833db282e1a075c526604f34e3158c",
        "id": "vlmSweOdLUb8"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "3b2b3ad5b592977b404acfa1c9ad303a62837255",
        "id": "NmEYgc8sLUb8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.save(KERAS_MODEL)\n",
        "w2v_model.save(WORD2VEC_MODEL)\n",
        "pickle.dump(tokenizer, open(TOKENIZER_MODEL, \"wb\"), protocol=0)\n",
        "pickle.dump(encoder, open(ENCODER_MODEL, \"wb\"), protocol=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "cc363c54782894757f5ea8820c6a170f2e16ef93",
        "id": "IxlAfbe1LUb9",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 2477,
          "sourceId": 4140,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 18195,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
